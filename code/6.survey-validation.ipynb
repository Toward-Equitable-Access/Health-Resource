{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the folder containing all Excel files\n",
    "folder_path = '../covid-validation-data'\n",
    "\n",
    "# List to store the extracted data\n",
    "data_list = []\n",
    "\n",
    "# Loop through all the Excel files in the folder\n",
    "for i in range(1, 34):  # Loop through health1_week1.xlsx to health1_week33.xlsx\n",
    "    file_name = f'health1_week{i}.xlsx'\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    \n",
    "    # Read the Excel file, skip the first rows to check if data starts further down\n",
    "    xl = pd.ExcelFile(file_path)\n",
    "    \n",
    "    # Loop through all the sheets (AL to WY) and extract the needed data\n",
    "    for sheet_name in xl.sheet_names:\n",
    "        if 'US' in sheet_name or 'Metro_Area' in sheet_name:\n",
    "            continue  # Skip sheets with 'US' or 'Metro_Area' in the name\n",
    "        \n",
    "        df = xl.parse(sheet_name, skiprows=4)  # Skip 4 rows to adjust header\n",
    "        \n",
    "        # Clean the column names\n",
    "        df.columns = df.columns.str.strip()  # Remove leading/trailing spaces\n",
    "        df.columns = df.columns.str.replace('\\n', '')  # Remove newlines\n",
    "\n",
    "        # Try to find the 'Total' row and corresponding columns for Yes/No data\n",
    "        try:\n",
    "            total_row = df[df.iloc[:, 0].str.contains('Total', na=False)]  # Locate the 'Total' row\n",
    "            \n",
    "            # Now identify the correct columns based on known structure (Yes/No for delayed and needed)\n",
    "            delayed_total_yes = total_row.iloc[0, 1]  # Yes is in the second column\n",
    "            delayed_total_no = total_row.iloc[0, 2]  # No is in the third column\n",
    "            \n",
    "            # Append the extracted data to the list\n",
    "            data_list.append({\n",
    "                'week': i,\n",
    "                'state': sheet_name,\n",
    "                'delayed total yes': delayed_total_yes,\n",
    "                'delayed total no': delayed_total_no\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sheet {sheet_name} in file {file_name}: {e}\")\n",
    "\n",
    "# Convert the data list to a df\n",
    "df_extracted = pd.DataFrame(data_list)\n",
    "df_extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the ratio: Delayed Total Yes / (Delayed Total Yes + Delayed Total No)\n",
    "df_extracted['delayed ratio'] = df_extracted['delayed total yes'] / (df_extracted['delayed total yes'] + df_extracted['delayed total no'])\n",
    "df_extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping weeks to their respective dates\n",
    "week_to_date = {\n",
    "    1: '2020-04-23', 2: '2020-05-07', 3: '2020-05-14', 4: '2020-05-21', 5: '2020-05-28',\n",
    "    6: '2020-06-04', 7: '2020-06-11', 8: '2020-06-18', 9: '2020-06-25', 10: '2020-07-02',\n",
    "    11: '2020-07-09', 12: '2020-07-16', 13: '2020-08-19', 14: '2020-09-02', 15: '2020-09-16',\n",
    "    16: '2020-09-30', 17: '2020-10-14', 18: '2020-10-28', 19: '2020-11-11', 20: '2020-11-25',\n",
    "    21: '2020-12-09', 22: '2021-01-06', 23: '2021-01-20', 24: '2021-02-03', 25: '2021-02-17',\n",
    "    26: '2021-03-03', 27: '2021-03-17', 28: '2021-04-14', 29: '2021-04-28', 30: '2021-05-12',\n",
    "    31: '2021-06-16', 32: '2021-06-31', 33: '2021-07-14'\n",
    "}\n",
    "\n",
    "# Adding the 'date' column based on the mapping\n",
    "df_extracted['date'] = df_extracted['week'].map(week_to_date)\n",
    "\n",
    "df_extracted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minus 4 Weeks Log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to subtract 4 weeks (28 days) from the date\n",
    "def subtract_4_weeks(date_str):\n",
    "    try:\n",
    "        # Convert the date string to a datetime object\n",
    "        date_obj = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "        # Subtract 28 days\n",
    "        new_date = date_obj - timedelta(weeks=4)\n",
    "        # Return the new date as a string\n",
    "        return new_date.strftime('%Y-%m-%d')\n",
    "    except ValueError:\n",
    "        # If there's an issue converting, return the original string\n",
    "        return date_str\n",
    "\n",
    "# Create a new DataFrame and subtract 4 weeks from the 'month' column\n",
    "df_extracted_minus_4 = df_extracted.copy()\n",
    "df_extracted_minus_4['date'] = df_extracted_minus_4['date'].apply(subtract_4_weeks)\n",
    "\n",
    "# Drop useless columns\n",
    "columns_to_drop = ['delayed total yes', 'delayed total no']\n",
    "df_extracted_minus_4 = df_extracted_minus_4.drop(columns=columns_to_drop)\n",
    "\n",
    "# Display the updated dataframe\n",
    "df_extracted_minus_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the code to filter the data for the period between 2020-04 and 2021-05\n",
    "\n",
    "# Define the function to extract year-month from 'month'\n",
    "def extract_year_month(date_str):\n",
    "    try:\n",
    "        # Convert to datetime object and extract year-month\n",
    "        return datetime.strptime(date_str, '%Y-%m-%d').strftime('%Y-%m')\n",
    "    except ValueError:\n",
    "        return date_str\n",
    "\n",
    "# Apply the function to extract 'month' as year-month in df_extracted_minus_4\n",
    "df_extracted_minus_4['month'] = df_extracted_minus_4['date'].apply(extract_year_month)\n",
    "\n",
    "# Filter the data for the date range 2020-04 to 2021-05\n",
    "df_filtered = df_extracted_minus_4[(df_extracted_minus_4['month'] >= '2020-04') & (df_extracted_minus_4['month'] <= '2021-05')]\n",
    "\n",
    "# Calculate the weighted average of 'delayed ratio' by grouping by 'state' and 'month'\n",
    "df_weighted_avg_minus_4 = df_filtered.groupby(['state', 'month']).apply(\n",
    "    lambda x: (x['delayed ratio'] * x['week']).sum() / x['week'].sum()\n",
    ").reset_index(name='weighted_delayed_ratio')\n",
    "\n",
    "df_weighted_avg_minus_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the sentiment_validation.csv dataset from the provided path\n",
    "sentiment_df = pd.read_csv('./sentiment_validation.csv')\n",
    "\n",
    "# Display the first few rows to ensure it loaded correctly\n",
    "sentiment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# State abbreviation to full name mapping\n",
    "state_abbr_to_name = {\n",
    "    'AL': 'Alabama', 'AK': 'Alaska', 'AZ': 'Arizona', 'AR': 'Arkansas', 'CA': 'California', 'CO': 'Colorado',\n",
    "    'CT': 'Connecticut', 'DE': 'Delaware', 'FL': 'Florida', 'GA': 'Georgia', 'HI': 'Hawaii', 'ID': 'Idaho',\n",
    "    'IL': 'Illinois', 'IN': 'Indiana', 'IA': 'Iowa', 'KS': 'Kansas', 'KY': 'Kentucky', 'LA': 'Louisiana',\n",
    "    'ME': 'Maine', 'MD': 'Maryland', 'MA': 'Massachusetts', 'MI': 'Michigan', 'MN': 'Minnesota', 'MS': 'Mississippi',\n",
    "    'MO': 'Missouri', 'MT': 'Montana', 'NE': 'Nebraska', 'NV': 'Nevada', 'NH': 'New Hampshire', 'NJ': 'New Jersey',\n",
    "    'NM': 'New Mexico', 'NY': 'New York', 'NC': 'North Carolina', 'ND': 'North Dakota', 'OH': 'Ohio', 'OK': 'Oklahoma',\n",
    "    'OR': 'Oregon', 'PA': 'Pennsylvania', 'RI': 'Rhode Island', 'SC': 'South Carolina', 'SD': 'South Dakota',\n",
    "    'TN': 'Tennessee', 'TX': 'Texas', 'UT': 'Utah', 'VT': 'Vermont', 'VA': 'Virginia', 'WA': 'Washington',\n",
    "    'WV': 'West Virginia', 'WI': 'Wisconsin', 'WY': 'Wyoming'\n",
    "}\n",
    "\n",
    "# Map state abbreviations to full names in df_weighted_avg_minus_4\n",
    "df_weighted_avg_minus_4['state'] = df_weighted_avg_minus_4['state'].map(state_abbr_to_name)\n",
    "\n",
    "# Merge sentiment_df and df_weighted_avg_minus_4 based on 'state' and 'month'\n",
    "merged_df = pd.merge(sentiment_df, df_weighted_avg_minus_4, on=['state', 'month'], how='inner')\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation between 'average_sentiment' and 'weighted_delayed_ratio'\n",
    "correlation_value = merged_df['average_sentiment'].corr(merged_df['weighted_delayed_ratio'])\n",
    "\n",
    "# Display the correlation value\n",
    "correlation_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting font properties\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "plt.rcParams['font.weight'] = 'bold'\n",
    "\n",
    "# Scatter plot for average_sentiment vs weighted_delayed_ratio\n",
    "plt.scatter(\n",
    "    merged_df['average_sentiment'], \n",
    "    merged_df['weighted_delayed_ratio'], \n",
    "    alpha=0.7, edgecolors='k', label='Data Points'\n",
    ")\n",
    "\n",
    "# Adding a trendline\n",
    "z = np.polyfit(merged_df['average_sentiment'], merged_df['weighted_delayed_ratio'], 1)\n",
    "p = np.poly1d(z)\n",
    "x_vals = np.linspace(merged_df['average_sentiment'].min(), merged_df['average_sentiment'].max(), 100)\n",
    "plt.plot(\n",
    "    x_vals, \n",
    "    p(x_vals), \n",
    "    color='red', linestyle='--', label='Correlation line'\n",
    ")\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.xlabel('Average Perception Score', fontweight='bold')\n",
    "plt.ylabel('Weighted Delayed Ratio', fontweight='bold')\n",
    "\n",
    "# Adding legend and grid\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the recommended threshold\n",
    "threshold = 4 / len(merged_df)  # Based on sample size\n",
    "print(f\"Cook's Distance threshold: {threshold}\")\n",
    "\n",
    "# Define function to remove high Cook's Distance points\n",
    "def remove_high_cooks_distance(df, independent_vars, dependent_var, threshold):\n",
    "    \"\"\"\n",
    "    Removes points with high Cook's Distance from a DataFrame.\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the data.\n",
    "    - independent_vars: List of independent variable column names.\n",
    "    - dependent_var: Dependent variable column name.\n",
    "    - threshold: Cook's Distance threshold (calculated externally).\n",
    "    Returns:\n",
    "    - Filtered DataFrame without high Cook's Distance points.\n",
    "    \"\"\"\n",
    "    # Extract independent variables (X) and dependent variable (y)\n",
    "    X = df[independent_vars]\n",
    "    y = df[dependent_var]\n",
    "    \n",
    "    # Ensure all values are numeric and drop NaNs\n",
    "    X = X.apply(pd.to_numeric, errors='coerce').dropna()\n",
    "    y = pd.to_numeric(y, errors='coerce').dropna()\n",
    "    \n",
    "    # Align indices of X and y after cleaning\n",
    "    X = X.loc[y.index]\n",
    "    y = y.loc[X.index]\n",
    "\n",
    "    # Add constant term for intercept\n",
    "    X = sm.add_constant(X)\n",
    "\n",
    "    # Fit OLS regression model\n",
    "    model = sm.OLS(y, X).fit()\n",
    "\n",
    "    # Calculate Cook's Distance\n",
    "    influence = model.get_influence()\n",
    "    cooks_d = influence.cooks_distance[0]  # Extract Cook's Distance values\n",
    "\n",
    "    # Add Cook's Distance to the DataFrame for filtering\n",
    "    df = df.iloc[:len(cooks_d)].copy()  # Align DataFrame with calculated Cook's Distance values\n",
    "    df.loc[:, 'cooks_d'] = cooks_d  # Safely add Cook's Distance column using .loc\n",
    "\n",
    "    # Filter out points with Cook's Distance above the threshold\n",
    "    filtered_df = df[df['cooks_d'] <= threshold].copy()\n",
    "\n",
    "    # Drop Cook's Distance column before returning\n",
    "    filtered_df.drop(columns=['cooks_d'], inplace=True)\n",
    "    return filtered_df\n",
    "\n",
    "# Define independent and dependent variables\n",
    "independent_vars = ['average_sentiment', 'weighted_delayed_ratio']  # Independent variables\n",
    "dependent_var = 'review_count'  # Replace with the actual dependent variable if applicable\n",
    "\n",
    "# Remove high Cook's Distance points using the calculated threshold\n",
    "filtered_df = remove_high_cooks_distance(merged_df, independent_vars, dependent_var, threshold)\n",
    "\n",
    "# Check the resulting DataFrame\n",
    "print(f\"Original DataFrame size: {merged_df.shape[0]}\")\n",
    "print(f\"Filtered DataFrame size: {filtered_df.shape[0]}\")\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation between 'average_sentiment' and 'weighted_delayed_ratio'\n",
    "correlation_value_1 = filtered_df['average_sentiment'].corr(filtered_df['weighted_delayed_ratio'])\n",
    "\n",
    "# Display the correlation value\n",
    "correlation_value_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set font properties\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "plt.rcParams['font.weight'] = 'bold'\n",
    "\n",
    "# Scatter plot for average_sentiment vs weighted_delayed_ratio\n",
    "plt.scatter(\n",
    "    filtered_df['average_sentiment'], \n",
    "    filtered_df['weighted_delayed_ratio'], \n",
    "    alpha=0.7, edgecolors='k', label='Data Points'\n",
    ")\n",
    "\n",
    "# Add a trendline\n",
    "z = np.polyfit(filtered_df['average_sentiment'], filtered_df['weighted_delayed_ratio'], 1)\n",
    "p = np.poly1d(z)\n",
    "x_vals = np.linspace(filtered_df['average_sentiment'].min(), filtered_df['average_sentiment'].max(), 100)\n",
    "plt.plot(\n",
    "    x_vals, \n",
    "    p(x_vals), \n",
    "    color='red', linestyle='--', label='Correlation line'\n",
    ")\n",
    "\n",
    "# Add titles and labels\n",
    "plt.xlabel('Average Perception Score', fontweight='bold')\n",
    "plt.ylabel('Weighted Delayed Ratio', fontweight='bold')\n",
    "\n",
    "# Add legend and grid\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
