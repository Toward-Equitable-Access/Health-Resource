{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d335d11",
   "metadata": {},
   "source": [
    "# 2. Text Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "186e03a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, KFold, StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "# import torch\n",
    "# from torch.utils.data import DataLoader, TensorDataset\n",
    "# from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37eff2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data = pd.read_csv('../filtered-labeled-data/labeled_data.csv')\n",
    "unlabeled_data = pd.read_csv('../filtered-labeled-data/unlabeled_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a1edab",
   "metadata": {},
   "source": [
    "## 2.1 TFIDF + Random Forest/ SVM/ Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cdc75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('../filtered-labeled-data/labeled_data.csv')\n",
    "\n",
    "# Only use labeled data and apply mapping for consistency\n",
    "labeled_data = data.dropna(subset=['class']).copy() # Use .copy() to avoid SettingWithCopyWarning\n",
    "\n",
    "# Map labels for consistency with BERT splits before splitting\n",
    "label_mapping = {-1: 0, 1: 1, 9: 2}\n",
    "# Ensure 'class' column is of a type that can be mapped (e.g. int)\n",
    "labeled_data['class'] = labeled_data['class'].astype(int).map(label_mapping)\n",
    "\n",
    "X = labeled_data['text']\n",
    "y = labeled_data['class'] # y now contains mapped labels (0, 1, 2)\n",
    "\n",
    "# Split into training and test sets (80-20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y # Stratification is now on mapped labels\n",
    ")\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf = TfidfVectorizer(stop_words='english', max_features=10000)\n",
    "\n",
    "# Define the pipeline for each classifier\n",
    "pipe_rf = Pipeline([\n",
    "    ('tfidf', tfidf),\n",
    "    ('clf', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "pipe_svm = Pipeline([\n",
    "    ('tfidf', tfidf),\n",
    "    ('clf', SVC(random_state=42, probability=True)) # Added probability=True for SVC if needed by some metrics, though not strictly for the current scorers\n",
    "])\n",
    "\n",
    "pipe_lr = Pipeline([\n",
    "    ('tfidf', tfidf),\n",
    "    ('clf', LogisticRegression(random_state=42, solver='liblinear')) # Changed solver for potentially better convergence with smaller datasets\n",
    "])\n",
    "\n",
    "# Define hyperparameter grids\n",
    "param_grid_rf = {\n",
    "    'clf__n_estimators': [100, 200], # Reduced for faster example\n",
    "    'clf__max_depth': [10, 20, None], # Reduced options\n",
    "    'clf__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "param_grid_svm = {\n",
    "    'clf__C': [0.1, 1, 10], # Reduced options\n",
    "    'clf__kernel': ['linear', 'rbf'],\n",
    "    'clf__gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "param_grid_lr = {\n",
    "    'clf__C': [0.1, 1, 10], # Reduced options\n",
    "    'clf__max_iter': [100, 200], # Adjusted max_iter\n",
    "    'clf__solver': ['liblinear', 'saga'] # Adjusted solvers\n",
    "}\n",
    "\n",
    "# Define scorers\n",
    "scorers = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'precision': make_scorer(precision_score, average='weighted', zero_division=0),\n",
    "    'recall': make_scorer(recall_score, average='weighted', zero_division=0),\n",
    "    'f1_score': make_scorer(f1_score, average='weighted', zero_division=0)\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV for each classifier\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42) # n_splits=5 is common\n",
    "grid_rf = GridSearchCV(pipe_rf, param_grid_rf, cv=kf, scoring=scorers, refit='f1_score', n_jobs=-1)\n",
    "grid_svm = GridSearchCV(pipe_svm, param_grid_svm, cv=kf, scoring=scorers, refit='f1_score', n_jobs=-1)\n",
    "grid_lr = GridSearchCV(pipe_lr, param_grid_lr, cv=kf, scoring=scorers, refit='f1_score', n_jobs=-1)\n",
    "\n",
    "# Fit the models\n",
    "print(\"Fitting Random Forest...\")\n",
    "grid_rf.fit(X_train, y_train)\n",
    "print(\"Fitting SVM...\")\n",
    "grid_svm.fit(X_train, y_train)\n",
    "print(\"Fitting Logistic Regression...\")\n",
    "grid_lr.fit(X_train, y_train)\n",
    "\n",
    "# Function to print formatted results for both training and test sets\n",
    "def print_results(name, grid, X_test_data, y_test_data): # Removed X_train, y_train as CV results are on grid object\n",
    "    print(f\"\\n{name} Results:\")\n",
    "    print(\"Cross-validation Results (on training data):\")\n",
    "    print(f\"  Best Score (F1): {grid.best_score_:.4f}\")\n",
    "    print(f\"  Best Parameters: {grid.best_params_}\")\n",
    "    \n",
    "    # Accessing CV results correctly\n",
    "    best_index = grid.best_index_\n",
    "    print(\"  Cross-validation Performance Metrics (for best estimator):\")\n",
    "    print(f\"    Mean CV Accuracy: {grid.cv_results_['mean_test_accuracy'][best_index]:.4f}\")\n",
    "    print(f\"    Mean CV Precision: {grid.cv_results_['mean_test_precision'][best_index]:.4f}\")\n",
    "    print(f\"    Mean CV Recall: {grid.cv_results_['mean_test_recall'][best_index]:.4f}\")\n",
    "    print(f\"    Mean CV F1 Score: {grid.cv_results_['mean_test_f1_score'][best_index]:.4f}\")\n",
    "    \n",
    "    # Calculate and print test set performance\n",
    "    y_pred_test = grid.predict(X_test_data)\n",
    "    print(\"\\nTest Set Performance:\")\n",
    "    print(f\"    Accuracy: {accuracy_score(y_test_data, y_pred_test):.4f}\")\n",
    "    print(f\"    Precision: {precision_score(y_test_data, y_pred_test, average='weighted', zero_division=0):.4f}\")\n",
    "    print(f\"    Recall: {recall_score(y_test_data, y_pred_test, average='weighted', zero_division=0):.4f}\")\n",
    "    print(f\"    F1 Score: {f1_score(y_test_data, y_pred_test, average='weighted', zero_division=0):.4f}\")\n",
    "\n",
    "# Print results for each model\n",
    "print_results(\"Random Forest\", grid_rf, X_test, y_test)\n",
    "print_results(\"SVM\", grid_svm, X_test, y_test)\n",
    "print_results(\"Logistic Regression\", grid_lr, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e1e9ea",
   "metadata": {},
   "source": [
    "## 2.2 Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c93e23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "import copy  # For saving the best model weights\n",
    "\n",
    "# Load and prepare data\n",
    "data_path = '../filtered-labeled-data/labeled_data.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "filtered_data = data.dropna(subset=['class'])\n",
    "label_mapping = {-1: 0, 1: 1, 9: 2}\n",
    "filtered_data['class'] = filtered_data['class'].map(label_mapping)\n",
    "\n",
    "# Split into training and test sets (80-20 split)\n",
    "train_data, test_data = train_test_split(\n",
    "    filtered_data, test_size=0.2, random_state=42, stratify=filtered_data['class']\n",
    ")\n",
    "\n",
    "def encode_data(tokenizer, data):\n",
    "    tokens = tokenizer.batch_encode_plus(\n",
    "        data['text'].tolist(),\n",
    "        max_length=256,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return tokens['input_ids'], tokens['attention_mask'], torch.tensor(data['class'].values)\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "            outputs = model(b_input_ids, attention_mask=b_input_mask)\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            \n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(b_labels.cpu().numpy())\n",
    "    \n",
    "    return (\n",
    "        accuracy_score(all_labels, all_predictions),\n",
    "        precision_score(all_labels, all_predictions, average='weighted'),\n",
    "        recall_score(all_labels, all_predictions, average='weighted'),\n",
    "        f1_score(all_labels, all_predictions, average='weighted')\n",
    "    )\n",
    "\n",
    "def print_results(cv_results, test_results):\n",
    "    print(\"\\nBERT Model Results:\")\n",
    "    print(\"Cross-validation Results:\")\n",
    "    print(f\"  Best Score (F1): {np.mean([x[3] for x in cv_results]):.4f}\")\n",
    "    print(\"  Cross-validation Performance Metrics:\")\n",
    "    print(f\"    Accuracy: {np.mean([x[0] for x in cv_results]):.4f}\")\n",
    "    print(f\"    Precision: {np.mean([x[1] for x in cv_results]):.4f}\")\n",
    "    print(f\"    Recall: {np.mean([x[2] for x in cv_results]):.4f}\")\n",
    "    print(f\"    F1 Score: {np.mean([x[3] for x in cv_results]):.4f}\")\n",
    "    \n",
    "    print(\"\\nTest Set Performance:\")\n",
    "    print(f\"    Accuracy: {test_results[0]:.4f}\")\n",
    "    print(f\"    Precision: {test_results[1]:.4f}\")\n",
    "    print(f\"    Recall: {test_results[2]:.4f}\")\n",
    "    print(f\"    F1 Score: {test_results[3]:.4f}\")\n",
    "\n",
    "# Initialize device and tokenizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Prepare test data\n",
    "test_inputs, test_masks, test_labels = encode_data(tokenizer, test_data)\n",
    "test_dataset = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# K-fold cross-validation\n",
    "# kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_results = []\n",
    "\n",
    "max_epochs = 10     # Maximum number of training epochs, can be adjusted\n",
    "patience = 2        # Early Stopping patience value, stop training when validation loss doesn't improve\n",
    "\n",
    "# for train_idx, val_idx in kf.split(train_data):\n",
    "for train_idx, val_idx in kf.split(train_data, train_data['class']):\n",
    "    # 1) Reinitialize the model at the start of each fold\n",
    "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3).to(device)\n",
    "    \n",
    "    # Prepare data for this fold\n",
    "    fold_train_data = train_data.iloc[train_idx]\n",
    "    fold_val_data = train_data.iloc[val_idx]\n",
    "    \n",
    "    train_inputs, train_masks, train_labels = encode_data(tokenizer, fold_train_data)\n",
    "    val_inputs, val_masks, val_labels = encode_data(tokenizer, fold_val_data)\n",
    "    \n",
    "    train_dataset = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=16)\n",
    "    val_dataset = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=16)\n",
    "    \n",
    "    # 2) Define optimizer and learning rate scheduler\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    total_steps = len(train_dataloader) * max_epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # Early Stopping variables\n",
    "    best_val_loss = float('inf')\n",
    "    wait = 0\n",
    "    best_model_weights = None\n",
    "    \n",
    "    # Training with Early Stopping\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        for batch in train_dataloader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "            \n",
    "            model.zero_grad()\n",
    "            outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # 3) Perform scheduler step after each parameter update\n",
    "            scheduler.step()\n",
    "        \n",
    "        # Calculate current epoch validation loss\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        for batch in val_dataloader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "            with torch.no_grad():\n",
    "                outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "                val_loss += outputs.loss.item()\n",
    "        val_loss /= len(val_dataloader)\n",
    "        \n",
    "        # Determine whether to update the best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            wait = 0\n",
    "            best_model_weights = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            wait += 1\n",
    "        \n",
    "        # Stop training if no improvement for patience epochs\n",
    "        if wait >= patience:\n",
    "            break\n",
    "    \n",
    "    # Load the best model weights from validation performance\n",
    "    if best_model_weights is not None:\n",
    "        model.load_state_dict(best_model_weights)\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    cv_results.append(evaluate(model, val_dataloader, device))\n",
    "\n",
    "# Final evaluation on test set\n",
    "test_results = evaluate(model, test_dataloader, device)\n",
    "\n",
    "# Print results in the same format as the previous code\n",
    "print_results(cv_results, test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f95950",
   "metadata": {},
   "source": [
    "## 2.3 ModernBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4219c0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade transformers\n",
    "# %pip install git+https://github.com/huggingface/transformers.git\n",
    "# %pip install flash-attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35788fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "import copy  # For saving the best model weights\n",
    "\n",
    "# Load and prepare data\n",
    "data_path = '../filtered-labeled-data/labeled_data.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "filtered_data = data.dropna(subset=['class'])\n",
    "\n",
    "# Map original labels to 0, 1, 2\n",
    "label_mapping = {-1: 0, 1: 1, 9: 2}\n",
    "filtered_data['class'] = filtered_data['class'].map(label_mapping)\n",
    "\n",
    "# Split into training and test sets (80-20 split)\n",
    "train_data, test_data = train_test_split(\n",
    "    filtered_data, test_size=0.2, random_state=42, stratify=filtered_data['class']\n",
    ")\n",
    "\n",
    "def encode_data(tokenizer, data):\n",
    "    tokens = tokenizer.batch_encode_plus(\n",
    "        data['text'].tolist(),\n",
    "        max_length=256,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return tokens['input_ids'], tokens['attention_mask'], torch.tensor(data['class'].values)\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "            outputs = model(b_input_ids, attention_mask=b_input_mask)\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            \n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(b_labels.cpu().numpy())\n",
    "    \n",
    "    return (\n",
    "        accuracy_score(all_labels, all_predictions),\n",
    "        precision_score(all_labels, all_predictions, average='weighted'),\n",
    "        recall_score(all_labels, all_predictions, average='weighted'),\n",
    "        f1_score(all_labels, all_predictions, average='weighted')\n",
    "    )\n",
    "\n",
    "def print_results(cv_results, test_results):\n",
    "    print(\"\\nModernBERT Model Results:\")\n",
    "    print(\"Cross-validation Results:\")\n",
    "    print(f\"  Best Score (F1): {np.mean([x[3] for x in cv_results]):.4f}\")\n",
    "    print(\"  Cross-validation Performance Metrics:\")\n",
    "    print(f\"    Accuracy: {np.mean([x[0] for x in cv_results]):.4f}\")\n",
    "    print(f\"    Precision: {np.mean([x[1] for x in cv_results]):.4f}\")\n",
    "    print(f\"    Recall: {np.mean([x[2] for x in cv_results]):.4f}\")\n",
    "    print(f\"    F1 Score: {np.mean([x[3] for x in cv_results]):.4f}\")\n",
    "    \n",
    "    print(\"\\nTest Set Performance:\")\n",
    "    print(f\"    Accuracy: {test_results[0]:.4f}\")\n",
    "    print(f\"    Precision: {test_results[1]:.4f}\")\n",
    "    print(f\"    Recall: {test_results[2]:.4f}\")\n",
    "    print(f\"    F1 Score: {test_results[3]:.4f}\")\n",
    "\n",
    "# Initialize device and tokenizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Use ModernBERT tokenizer\n",
    "model_name = \"answerdotai/ModernBERT-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Prepare test data\n",
    "test_inputs, test_masks, test_labels = encode_data(tokenizer, test_data)\n",
    "test_dataset = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# K-fold cross-validation\n",
    "# kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_results = []\n",
    "\n",
    "max_epochs = 10     # Maximum number of training epochs, can be adjusted\n",
    "patience = 2        # Early Stopping patience value, stop training when validation loss doesn't improve\n",
    "\n",
    "# for train_idx, val_idx in kf.split(train_data):\n",
    "for train_idx, val_idx in kf.split(train_data, train_data['class']):\n",
    "\n",
    "    # 1) Reinitialize the model at the start of each fold using ModernBERT\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=3\n",
    "    ).to(device)\n",
    "    \n",
    "    # Prepare data for this fold\n",
    "    fold_train_data = train_data.iloc[train_idx]\n",
    "    fold_val_data = train_data.iloc[val_idx]\n",
    "    \n",
    "    train_inputs, train_masks, train_labels = encode_data(tokenizer, fold_train_data)\n",
    "    val_inputs, val_masks, val_labels = encode_data(tokenizer, fold_val_data)\n",
    "    \n",
    "    train_dataset = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=16)\n",
    "    val_dataset = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=16)\n",
    "    \n",
    "    # 2) Define optimizer and learning rate scheduler\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    total_steps = len(train_dataloader) * max_epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0,\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "    \n",
    "    # Early Stopping variables\n",
    "    best_val_loss = float('inf')\n",
    "    wait = 0\n",
    "    best_model_weights = None\n",
    "    \n",
    "    # Training with Early Stopping\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        for batch in train_dataloader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "            \n",
    "            model.zero_grad()\n",
    "            outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # 3) Perform scheduler step after each parameter update\n",
    "            scheduler.step()\n",
    "        \n",
    "        # Calculate current epoch validation loss\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        for batch in val_dataloader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "            with torch.no_grad():\n",
    "                outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "                val_loss += outputs.loss.item()\n",
    "        val_loss /= len(val_dataloader)\n",
    "        \n",
    "        # Determine whether to update the best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            wait = 0\n",
    "            best_model_weights = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            wait += 1\n",
    "        \n",
    "        # Stop training if no improvement for patience epochs\n",
    "        if wait >= patience:\n",
    "            break\n",
    "    \n",
    "    # Load the best model weights from validation performance\n",
    "    if best_model_weights is not None:\n",
    "        model.load_state_dict(best_model_weights)\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    cv_results.append(evaluate(model, val_dataloader, device))\n",
    "\n",
    "# Final evaluation on test set\n",
    "test_results = evaluate(model, test_dataloader, device)\n",
    "\n",
    "# Print results\n",
    "print_results(cv_results, test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0220bbdd",
   "metadata": {},
   "source": [
    "## 2.4 RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97c4a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold # Changed KFold to StratifiedKFold for consistency\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "import copy  # For saving the best model weights\n",
    "\n",
    "# Load and prepare data\n",
    "data_path = '../filtered-labeled-data/labeled_data.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "filtered_data = data.dropna(subset=['class'])\n",
    "\n",
    "# Map original labels to 0, 1, 2\n",
    "label_mapping = {-1: 0, 1: 1, 9: 2}\n",
    "filtered_data['class'] = filtered_data['class'].map(label_mapping)\n",
    "\n",
    "# Split into training and test sets (80-20 split)\n",
    "train_data, test_data = train_test_split(\n",
    "    filtered_data, test_size=0.2, random_state=42, stratify=filtered_data['class']\n",
    ")\n",
    "\n",
    "def encode_data(tokenizer, data):\n",
    "    tokens = tokenizer.batch_encode_plus(\n",
    "        data['text'].tolist(),\n",
    "        max_length=256,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return tokens['input_ids'], tokens['attention_mask'], torch.tensor(data['class'].values)\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "            outputs = model(b_input_ids, attention_mask=b_input_mask)\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(b_labels.cpu().numpy())\n",
    "\n",
    "    return (\n",
    "        accuracy_score(all_labels, all_predictions),\n",
    "        precision_score(all_labels, all_predictions, average='weighted', zero_division=0), # Added zero_division\n",
    "        recall_score(all_labels, all_predictions, average='weighted', zero_division=0),  # Added zero_division\n",
    "        f1_score(all_labels, all_predictions, average='weighted', zero_division=0)       # Added zero_division\n",
    "    )\n",
    "\n",
    "def print_results(cv_results, test_results):\n",
    "    print(\"\\nRoBERTa Model Results:\") # Changed model name\n",
    "    print(\"Cross-validation Results:\")\n",
    "    # Calculate mean of F1 scores from CV, ensuring cv_results is not empty\n",
    "    mean_f1_cv = np.mean([x[3] for x in cv_results]) if cv_results else float('nan')\n",
    "    print(f\"  Mean F1 Score (CV): {mean_f1_cv:.4f}\")\n",
    "    # print(f\"  Best Score (F1): {np.mean([x[3] for x in cv_results]):.4f}\") # Original line, if preferred\n",
    "    print(\"  Cross-validation Performance Metrics (Mean):\")\n",
    "    mean_acc_cv = np.mean([x[0] for x in cv_results]) if cv_results else float('nan')\n",
    "    mean_prec_cv = np.mean([x[1] for x in cv_results]) if cv_results else float('nan')\n",
    "    mean_rec_cv = np.mean([x[2] for x in cv_results]) if cv_results else float('nan')\n",
    "    print(f\"    Accuracy: {mean_acc_cv:.4f}\")\n",
    "    print(f\"    Precision: {mean_prec_cv:.4f}\")\n",
    "    print(f\"    Recall: {mean_rec_cv:.4f}\")\n",
    "    print(f\"    F1 Score: {mean_f1_cv:.4f}\") # Re-iterating mean F1 for clarity\n",
    "\n",
    "    print(\"\\nTest Set Performance:\")\n",
    "    print(f\"    Accuracy: {test_results[0]:.4f}\")\n",
    "    print(f\"    Precision: {test_results[1]:.4f}\")\n",
    "    print(f\"    Recall: {test_results[2]:.4f}\")\n",
    "    print(f\"    F1 Score: {test_results[3]:.4f}\")\n",
    "\n",
    "# Initialize device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Use RoBERTa tokenizer and model\n",
    "model_name = \"roberta-base\"  # Changed to RoBERTa\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Prepare test data\n",
    "test_inputs, test_masks, test_labels = encode_data(tokenizer, test_data)\n",
    "test_dataset = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16) # Consistent batch size\n",
    "\n",
    "# K-fold cross-validation (using StratifiedKFold for consistency)\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_results = [] # Stores (accuracy, precision, recall, f1) for each fold's validation\n",
    "\n",
    "max_epochs = 10     # Maximum number of training epochs, can be adjusted\n",
    "patience = 2        # Early Stopping patience value, stop training when validation loss doesn't improve\n",
    "learning_rate = 2e-5 # Standard learning rate\n",
    "\n",
    "# To store the best model across all folds based on validation F1 score\n",
    "best_fold_model_state = None\n",
    "best_fold_f1_score = -1.0\n",
    "fold_count = 0\n",
    "\n",
    "for train_idx, val_idx in kf.split(train_data, train_data['class']): # Ensure stratification for CV splits\n",
    "    fold_count += 1\n",
    "    print(f\"\\n--- Fold {fold_count}/5 ---\")\n",
    "    # 1) Reinitialize the model at the start of each fold using RoBERTa\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=3  # Assuming 3 classes based on label_mapping\n",
    "    ).to(device)\n",
    "\n",
    "    # Prepare data for this fold\n",
    "    fold_train_data = train_data.iloc[train_idx]\n",
    "    fold_val_data = train_data.iloc[val_idx]\n",
    "\n",
    "    train_inputs, train_masks, train_labels = encode_data(tokenizer, fold_train_data)\n",
    "    val_inputs, val_masks, val_labels = encode_data(tokenizer, fold_val_data)\n",
    "\n",
    "    train_dataset = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True) # Added shuffle=True for training\n",
    "    val_dataset = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=16) # Consistent batch size\n",
    "\n",
    "    # 2) Define optimizer and learning rate scheduler\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    total_steps = len(train_dataloader) * max_epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=0, # Can be adjusted, e.g., 0.1 * total_steps\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    # Early Stopping variables for the current fold\n",
    "    best_val_loss_fold = float('inf')\n",
    "    wait_fold = 0\n",
    "    best_model_weights_fold = None\n",
    "\n",
    "    # Training with Early Stopping for the current fold\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for batch in train_dataloader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "            model.zero_grad()\n",
    "            outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "            loss = outputs.loss\n",
    "            total_train_loss += loss.item() # Accumulate training loss\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # Gradient clipping\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader) # Average training loss\n",
    "\n",
    "        # Calculate current epoch validation loss and metrics\n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        val_preds_fold = []\n",
    "        val_labels_fold = []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                batch_device = tuple(t.to(device) for t in batch)\n",
    "                b_input_ids, b_input_mask, b_labels = batch_device\n",
    "                outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "                total_val_loss += outputs.loss.item()\n",
    "                logits = outputs.logits\n",
    "                val_preds_fold.extend(torch.argmax(logits, dim=-1).cpu().numpy())\n",
    "                val_labels_fold.extend(b_labels.cpu().numpy())\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_dataloader) # Average validation loss\n",
    "        val_f1_fold = f1_score(val_labels_fold, val_preds_fold, average='weighted', zero_division=0)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{max_epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val F1: {val_f1_fold:.4f}\")\n",
    "\n",
    "        # Determine whether to update the best model for this fold\n",
    "        if avg_val_loss < best_val_loss_fold: # Using val_loss for early stopping criterion\n",
    "            best_val_loss_fold = avg_val_loss\n",
    "            wait_fold = 0\n",
    "            best_model_weights_fold = copy.deepcopy(model.state_dict())\n",
    "            print(f\"Validation loss improved for fold {fold_count}. Saving model for epoch {epoch+1}\")\n",
    "        else:\n",
    "            wait_fold += 1\n",
    "            print(f\"Validation loss did not improve for {wait_fold} epoch(s) for fold {fold_count}.\")\n",
    "\n",
    "        # Stop training if no improvement for patience epochs\n",
    "        if wait_fold >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs for fold {fold_count}.\")\n",
    "            break\n",
    "\n",
    "    # Load the best model weights for this fold based on validation performance\n",
    "    if best_model_weights_fold is not None:\n",
    "        model.load_state_dict(best_model_weights_fold)\n",
    "        print(f\"Loaded best model weights for fold {fold_count} based on validation performance.\")\n",
    "\n",
    "    # Evaluate on validation set for this fold (using the best model for this fold)\n",
    "    fold_val_metrics = evaluate(model, val_dataloader, device) # (acc, prec, rec, f1)\n",
    "    cv_results.append(fold_val_metrics)\n",
    "    print(f\"Fold {fold_count} Validation Metrics: Acc: {fold_val_metrics[0]:.4f}, Prec: {fold_val_metrics[1]:.4f}, Rec: {fold_val_metrics[2]:.4f}, F1: {fold_val_metrics[3]:.4f}\")\n",
    "\n",
    "    # Check if this fold's model is the best overall based on validation F1\n",
    "    if fold_val_metrics[3] > best_fold_f1_score:\n",
    "        best_fold_f1_score = fold_val_metrics[3]\n",
    "        best_fold_model_state = copy.deepcopy(model.state_dict())\n",
    "        print(f\"*** New best model found in fold {fold_count} with Val F1: {best_fold_f1_score:.4f} ***\")\n",
    "\n",
    "\n",
    "# After all folds, load the best model state (from the fold with the highest validation F1) for final test set evaluation.\n",
    "if best_fold_model_state is not None:\n",
    "    print(f\"\\nLoading the best model overall (from fold with Val F1: {best_fold_f1_score:.4f}) for final test evaluation...\")\n",
    "    model.load_state_dict(best_fold_model_state)\n",
    "else:\n",
    "    print(\"\\nNo best model state saved (e.g., if training was very short or had issues). Using model from the last fold for testing.\")\n",
    "    # The 'model' variable will hold the model from the last completed fold.\n",
    "\n",
    "print(\"\\nEvaluating on the test set using the best model from cross-validation...\")\n",
    "# Final evaluation on test set\n",
    "test_results_tuple = evaluate(model, test_dataloader, device) # Returns a tuple\n",
    "\n",
    "# Print results\n",
    "print_results(cv_results, test_results_tuple) # cv_results is a list of tuples, test_results_tuple is a single tuple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8166b897",
   "metadata": {},
   "source": [
    "## 2.5 DeBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823e903d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "import copy  # For saving the best model weights\n",
    "\n",
    "# Load and prepare data\n",
    "data_path = '../filtered-labeled-data/labeled_data.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "filtered_data = data.dropna(subset=['class'])\n",
    "\n",
    "# Map original labels to 0, 1, 2\n",
    "label_mapping = {-1: 0, 1: 1, 9: 2}\n",
    "filtered_data['class'] = filtered_data['class'].map(label_mapping)\n",
    "\n",
    "# Split into training and test sets (80-20 split)\n",
    "train_data, test_data = train_test_split(\n",
    "    filtered_data, test_size=0.2, random_state=42, stratify=filtered_data['class']\n",
    ")\n",
    "\n",
    "def encode_data(tokenizer, data):\n",
    "    tokens = tokenizer.batch_encode_plus(\n",
    "        data['text'].tolist(),\n",
    "        max_length=256, # DeBERTa can handle longer, but 256 is often a good balance\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return tokens['input_ids'], tokens['attention_mask'], torch.tensor(data['class'].values)\n",
    "\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "            outputs = model(b_input_ids, attention_mask=b_input_mask)\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(b_labels.cpu().numpy())\n",
    "\n",
    "    return (\n",
    "        accuracy_score(all_labels, all_predictions),\n",
    "        precision_score(all_labels, all_predictions, average='weighted', zero_division=0),\n",
    "        recall_score(all_labels, all_predictions, average='weighted', zero_division=0),\n",
    "        f1_score(all_labels, all_predictions, average='weighted', zero_division=0)\n",
    "    )\n",
    "\n",
    "def print_results(model_display_name, cv_results, test_results_tuple):\n",
    "    print(f\"\\n{model_display_name} Model Results:\")\n",
    "    print(\"Cross-validation Results:\")\n",
    "    mean_f1_cv = np.mean([x[3] for x in cv_results]) if cv_results else float('nan')\n",
    "    print(f\"  Mean F1 Score (CV): {mean_f1_cv:.4f}\")\n",
    "    print(\"  Cross-validation Performance Metrics (Mean):\")\n",
    "    mean_acc_cv = np.mean([x[0] for x in cv_results]) if cv_results else float('nan')\n",
    "    mean_prec_cv = np.mean([x[1] for x in cv_results]) if cv_results else float('nan')\n",
    "    mean_rec_cv = np.mean([x[2] for x in cv_results]) if cv_results else float('nan')\n",
    "    print(f\"    Accuracy: {mean_acc_cv:.4f}\")\n",
    "    print(f\"    Precision: {mean_prec_cv:.4f}\")\n",
    "    print(f\"    Recall: {mean_rec_cv:.4f}\")\n",
    "    print(f\"    F1 Score: {mean_f1_cv:.4f}\")\n",
    "\n",
    "    print(\"\\nTest Set Performance:\")\n",
    "    print(f\"    Accuracy: {test_results_tuple[0]:.4f}\")\n",
    "    print(f\"    Precision: {test_results_tuple[1]:.4f}\")\n",
    "    print(f\"    Recall: {test_results_tuple[2]:.4f}\")\n",
    "    print(f\"    F1 Score: {test_results_tuple[3]:.4f}\")\n",
    "\n",
    "# Initialize device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- DeBERTa Model Configuration ---\n",
    "model_name = \"microsoft/deberta-v3-base\"\n",
    "model_display_name = \"DeBERTa-v3-base\"\n",
    "# You could also try other variants like \"microsoft/deberta-base\"\n",
    "# model_name = \"microsoft/deberta-base\"\n",
    "# model_display_name = \"DeBERTa-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Prepare test data\n",
    "test_inputs, test_masks, test_labels = encode_data(tokenizer, test_data)\n",
    "test_dataset = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# K-fold cross-validation\n",
    "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_results = []\n",
    "\n",
    "max_epochs = 10\n",
    "patience = 2\n",
    "# DeBERTa models, especially v3, often benefit from a slightly lower learning rate.\n",
    "learning_rate = 1e-5  # Try 1e-5 for DeBERTa-v3\n",
    "# learning_rate = 2e-5 # Standard LR, could also work\n",
    "\n",
    "best_fold_model_state = None\n",
    "best_fold_f1_score = -1.0\n",
    "fold_count = 0\n",
    "\n",
    "for train_idx, val_idx in kf.split(train_data, train_data['class']):\n",
    "    fold_count += 1\n",
    "    print(f\"\\n--- Fold {fold_count}/5 ({model_display_name}) ---\")\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=3\n",
    "    ).to(device)\n",
    "\n",
    "    fold_train_data = train_data.iloc[train_idx]\n",
    "    fold_val_data = train_data.iloc[val_idx]\n",
    "\n",
    "    train_inputs, train_masks, train_labels = encode_data(tokenizer, fold_train_data)\n",
    "    val_inputs, val_masks, val_labels = encode_data(tokenizer, fold_val_data)\n",
    "\n",
    "    train_dataset = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_dataset = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=16)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    total_steps = len(train_dataloader) * max_epochs\n",
    "    # Using a small portion of total steps for warmup is common\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=int(0.1 * total_steps), # e.g., 10% of total steps for warmup\n",
    "        num_training_steps=total_steps\n",
    "    )\n",
    "\n",
    "    best_val_loss_fold = float('inf')\n",
    "    wait_fold = 0\n",
    "    best_model_weights_fold = None\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for batch in train_dataloader:\n",
    "            batch_to_device = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_mask, b_labels = batch_to_device\n",
    "            model.zero_grad()\n",
    "            outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "            loss = outputs.loss\n",
    "            total_train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # Gradient clipping\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "\n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        val_preds_fold = []\n",
    "        val_labels_fold = []\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                batch_to_device = tuple(t.to(device) for t in batch)\n",
    "                b_input_ids, b_input_mask, b_labels = batch_to_device\n",
    "                outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "                total_val_loss += outputs.loss.item()\n",
    "                logits = outputs.logits\n",
    "                val_preds_fold.extend(torch.argmax(logits, dim=-1).cpu().numpy())\n",
    "                val_labels_fold.extend(b_labels.cpu().numpy())\n",
    "        avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "        val_f1_fold = f1_score(val_labels_fold, val_preds_fold, average='weighted', zero_division=0)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{max_epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f} | Val F1: {val_f1_fold:.4f}\")\n",
    "\n",
    "        if avg_val_loss < best_val_loss_fold: # Early stopping based on validation loss\n",
    "            best_val_loss_fold = avg_val_loss\n",
    "            wait_fold = 0\n",
    "            best_model_weights_fold = copy.deepcopy(model.state_dict())\n",
    "            print(f\"Validation loss improved for fold {fold_count}. Saving model for epoch {epoch+1}\")\n",
    "        else:\n",
    "            wait_fold += 1\n",
    "            print(f\"Validation loss did not improve for {wait_fold} epoch(s) for fold {fold_count}.\")\n",
    "\n",
    "        if wait_fold >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs for fold {fold_count}.\")\n",
    "            break\n",
    "            \n",
    "    if best_model_weights_fold is not None:\n",
    "        model.load_state_dict(best_model_weights_fold)\n",
    "        print(f\"Loaded best model weights for fold {fold_count} based on validation performance.\")\n",
    "\n",
    "    fold_val_metrics = evaluate(model, val_dataloader, device) # (acc, prec, rec, f1)\n",
    "    cv_results.append(fold_val_metrics)\n",
    "    print(f\"Fold {fold_count} Validation Metrics: Acc: {fold_val_metrics[0]:.4f}, Prec: {fold_val_metrics[1]:.4f}, Rec: {fold_val_metrics[2]:.4f}, F1: {fold_val_metrics[3]:.4f}\")\n",
    "\n",
    "    # Check if this fold's model is the best overall based on validation F1\n",
    "    if fold_val_metrics[3] > best_fold_f1_score:\n",
    "        best_fold_f1_score = fold_val_metrics[3]\n",
    "        best_fold_model_state = copy.deepcopy(model.state_dict())\n",
    "        print(f\"*** New best model found in fold {fold_count} with Val F1: {best_fold_f1_score:.4f} ***\")\n",
    "\n",
    "# After all folds, load the best model state for final test set evaluation.\n",
    "if best_fold_model_state is not None:\n",
    "    print(f\"\\nLoading the best {model_display_name} model overall (from fold with Val F1: {best_fold_f1_score:.4f}) for final test evaluation...\")\n",
    "    model.load_state_dict(best_fold_model_state)\n",
    "else:\n",
    "    print(f\"\\nNo best {model_display_name} model state saved (e.g., if training was too short or had issues). Using model from the last fold for testing.\")\n",
    "\n",
    "print(f\"\\nEvaluating on the test set using the best {model_display_name} model from cross-validation...\")\n",
    "test_results_tuple = evaluate(model, test_dataloader, device)\n",
    "\n",
    "# Print final results\n",
    "print_results(model_display_name, cv_results, test_results_tuple)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15539f67",
   "metadata": {},
   "source": [
    "## 2.6 Classifiers Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb455c7f",
   "metadata": {},
   "source": [
    "```python\n",
    "BERT Model Results:\n",
    "Cross-validation Results:\n",
    "  Best Score (F1): 0.8760\n",
    "  Cross-validation Performance Metrics:\n",
    "    Accuracy: 0.8765\n",
    "    Precision: 0.8790\n",
    "    Recall: 0.8765\n",
    "    F1 Score: 0.8760\n",
    "\n",
    "Test Set Performance:\n",
    "    Accuracy: 0.8523\n",
    "    Precision: 0.8579\n",
    "    Recall: 0.8523\n",
    "    F1 Score: 0.8525\n",
    "\n",
    "RoBERTa Model Results:\n",
    "Cross-validation Results:\n",
    "  Mean F1 Score (CV): 0.8793\n",
    "  Cross-validation Performance Metrics (Mean):\n",
    "    Accuracy: 0.8789\n",
    "    Precision: 0.8830\n",
    "    Recall: 0.8789\n",
    "    F1 Score: 0.8793\n",
    "\n",
    "Test Set Performance:\n",
    "    Accuracy: 0.8646\n",
    "    Precision: 0.8666\n",
    "    Recall: 0.8646\n",
    "    F1 Score: 0.8644\n",
    "\n",
    "DeBERTa-v3-base Model Results:\n",
    "Cross-validation Results:\n",
    "  Mean F1 Score (CV): 0.8873\n",
    "  Cross-validation Performance Metrics (Mean):\n",
    "    Accuracy: 0.8868\n",
    "    Precision: 0.8892\n",
    "    Recall: 0.8855\n",
    "    F1 Score: 0.8873\n",
    "\n",
    "Test Set Performance:\n",
    "    Accuracy: 0.8705\n",
    "    Precision: 0.8715\n",
    "    Recall: 0.8700\n",
    "    F1 Score: 0.8709\n",
    "\n",
    "ModernBERT Model Results:\n",
    "Cross-validation Results:\n",
    "  Best Score (F1): 0.8391\n",
    "  Cross-validation Performance Metrics:\n",
    "    Accuracy: 0.8418\n",
    "    Precision: 0.8456\n",
    "    Recall: 0.8418\n",
    "    F1 Score: 0.8391\n",
    "\n",
    "Test Set Performance:\n",
    "    Accuracy: 0.8492\n",
    "    Precision: 0.8508\n",
    "    Recall: 0.8492\n",
    "    F1 Score: 0.8486\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d07dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import matplotlib.ticker as ticker\n",
    "# Set global font properties\n",
    "font_props = FontProperties(family='Times New Roman', weight='bold')\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "plt.rcParams['font.weight'] = 'bold'\n",
    "plt.rcParams['font.size'] = 24\n",
    "\n",
    "# Create DataFrames for each metric\n",
    "precision_data = pd.DataFrame({\n",
    "    'Model': ['BERT', 'RoBERTa', 'DeBERTa', 'ModernBERT'],\n",
    "    'CV': [0.8790, 0.8830, 0.8892, 0.8456],\n",
    "    'Test': [0.8579, 0.8666, 0.8715, 0.8508]\n",
    "})\n",
    "\n",
    "recall_data = pd.DataFrame({\n",
    "    'Model': ['BERT', 'RoBERTa', 'DeBERTa', 'ModernBERT'],\n",
    "    'CV': [0.8765, 0.8789, 0.8855, 0.8418],\n",
    "    'Test': [0.8523, 0.8646, 0.8700, 0.8492]\n",
    "})\n",
    "\n",
    "f1_data = pd.DataFrame({\n",
    "    'Model': ['BERT', 'RoBERTa', 'DeBERTa', 'ModernBERT'],\n",
    "    'CV': [0.8760, 0.8793, 0.8873, 0.8391],\n",
    "    'Test': [0.8525, 0.8644, 0.8709, 0.8486]\n",
    "})\n",
    "\n",
    "accuracy_data = pd.DataFrame({\n",
    "    'Model': ['BERT', 'RoBERTa', 'DeBERTa', 'ModernBERT'],\n",
    "    'CV': [0.8765, 0.8789, 0.8868, 0.8418],\n",
    "    'Test': [0.8523, 0.8646, 0.8705, 0.8492]\n",
    "})\n",
    "\n",
    "# Create figure with 2x2 subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "plt.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "\n",
    "def create_heatmap(data, ax, title, vmin=0.81, vmax=0.88):  # Adjusted vmax to accommodate BERT's higher values\n",
    "    # Create the heatmap\n",
    "    heatmap = sns.heatmap(data.set_index('Model'), annot=True, cmap='Blues', \n",
    "                         fmt='.3f', vmin=vmin, vmax=vmax, ax=ax, cbar=True,\n",
    "                         annot_kws={'size': 24, 'font': font_props})\n",
    "    \n",
    "    # Format colorbar\n",
    "    colorbar = heatmap.collections[0].colorbar\n",
    "    colorbar.set_ticks(ticker.LinearLocator(5))\n",
    "    colorbar.set_ticklabels([f'{x:.3f}' for x in colorbar.get_ticks()])\n",
    "    colorbar.ax.tick_params(labelsize=24)\n",
    "    for label in colorbar.ax.get_yticklabels():\n",
    "        label.set_font_properties(font_props)\n",
    "\n",
    "    # Configure y-axis labels\n",
    "    if ax in [axes[0,0], axes[1,0]]:\n",
    "        ax.set_ylabel('', fontsize=24, font=font_props)\n",
    "        ax.tick_params(axis='y', which='major', labelsize=24)\n",
    "        labels = ax.get_yticklabels()\n",
    "        ax.set_yticklabels(labels, rotation=0, font=font_props)\n",
    "    else:\n",
    "        ax.set_ylabel('')\n",
    "        ax.set_yticklabels([])\n",
    "\n",
    "    # Configure x-axis labels\n",
    "    ax.tick_params(axis='x', which='major', labelsize=24)\n",
    "    for label in ax.get_xticklabels():\n",
    "        label.set_font_properties(font_props)\n",
    "\n",
    "    # Set title\n",
    "    ax.set_title(title, pad=20, font=font_props)\n",
    "\n",
    "# Create heatmaps\n",
    "create_heatmap(precision_data, axes[0,0], 'a. Precision')\n",
    "create_heatmap(recall_data, axes[0,1], 'b. Recall')\n",
    "create_heatmap(f1_data, axes[1,0], 'c. F1-Score')\n",
    "create_heatmap(accuracy_data, axes[1,1], 'd. Accuracy')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('classifiers.pdf', format='pdf', bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59279a5",
   "metadata": {},
   "source": [
    "## 2.7 Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2a4503",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "import copy  # For saving the best model weights\n",
    "\n",
    "\n",
    "# --- Basic Configuration ---\n",
    "MODEL_NAME = \"microsoft/deberta-v3-base\"\n",
    "MODEL_DISPLAY_NAME = \"DeBERTa-v3-base\"\n",
    "LABELED_DATA_PATH = '../filtered-labeled-data/labeled_data.csv'\n",
    "UNLABELED_DATA_PATH = '../filtered-labeled-data/unlabeled_data.csv'\n",
    "MERGED_OUTPUT_FILENAME = '../filtered-labeled-data/classed_data.csv' # Updated filename for clarity\n",
    "\n",
    "BATCH_SIZE_TRAIN_VAL = 16\n",
    "BATCH_SIZE_PREDICT = 32\n",
    "MAX_LENGTH = 256\n",
    "LEARNING_RATE = 1e-5\n",
    "MAX_EPOCHS = 10\n",
    "PATIENCE = 2\n",
    "VALIDATION_SET_SIZE = 0.1\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# --- Device Setup ---\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Tokenizer Initialization ---\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    print(f\"Tokenizer ({MODEL_NAME}) initialized successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: Failed to initialize Tokenizer: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- 1. Load and Preprocess Labeled Data (to be used for training and validation) ---\n",
    "print(f\"Loading labeled data from: {LABELED_DATA_PATH}\")\n",
    "try:\n",
    "    labeled_data_full_original_cols = pd.read_csv(LABELED_DATA_PATH)\n",
    "    print(f\"Labeled data loaded successfully. Contains {len(labeled_data_full_original_cols)} records.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Labeled data file not found at {LABELED_DATA_PATH}\")\n",
    "    raise\n",
    "\n",
    "labeled_data_for_training = labeled_data_full_original_cols.copy()\n",
    "labeled_data_for_training.dropna(subset=['class'], inplace=True)\n",
    "label_mapping = {-1: 0, 1: 1, 9: 2}\n",
    "labeled_data_for_training['class'] = labeled_data_for_training['class'].map(label_mapping)\n",
    "labeled_data_for_training.dropna(subset=['class'], inplace=True)\n",
    "labeled_data_for_training['class'] = labeled_data_for_training['class'].astype(int)\n",
    "print(f\"Number of labeled records for training after preprocessing: {len(labeled_data_for_training)}\")\n",
    "if 'text' not in labeled_data_for_training.columns:\n",
    "    print(\"Error: 'text' column not found in labeled data.\")\n",
    "    raise ValueError(\"'text' column is required in labeled data.\")\n",
    "labeled_data_for_training.dropna(subset=['text'], inplace=True)\n",
    "print(f\"Number of labeled records for training after removing NaN values in 'text' column: {len(labeled_data_for_training)}\")\n",
    "\n",
    "# --- 2. Load Unlabeled Data (to be used for prediction) ---\n",
    "print(f\"Loading unlabeled data from: {UNLABELED_DATA_PATH}\")\n",
    "try:\n",
    "    unlabeled_df = pd.read_csv(UNLABELED_DATA_PATH)\n",
    "    print(f\"Unlabeled data loaded successfully. Contains {len(unlabeled_df)} records.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Unlabeled data file not found at {UNLABELED_DATA_PATH}\")\n",
    "    raise\n",
    "if 'text' not in unlabeled_df.columns:\n",
    "    print(\"Error: 'text' column not found in unlabeled data.\")\n",
    "    raise ValueError(\"'text' column not found in unlabeled data.\")\n",
    "unlabeled_df.dropna(subset=['text'], inplace=True)\n",
    "original_unlabeled_count = len(unlabeled_df)\n",
    "unlabeled_df_original_cols = unlabeled_df.copy() \n",
    "print(f\"Unlabeled data after cleaning (removing NaN values in 'text' column) has {original_unlabeled_count} records.\")\n",
    "\n",
    "# --- Helper Function: Encode Data ---\n",
    "def encode_data_with_labels(tokenizer_obj, data_df, max_len=MAX_LENGTH):\n",
    "    print(f\"Starting to encode {len(data_df)} texts for labeled data...\")\n",
    "    tokens = tokenizer_obj.batch_encode_plus(\n",
    "        data_df['text'].tolist(), max_length=max_len, padding='max_length', truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    print(\"Text encoding completed.\")\n",
    "    return tokens['input_ids'], tokens['attention_mask'], torch.tensor(data_df['class'].values)\n",
    "\n",
    "def encode_data_for_prediction(tokenizer_obj, texts_list, max_len=MAX_LENGTH):\n",
    "    print(f\"Starting to encode {len(texts_list)} texts for prediction...\")\n",
    "    encoded_output = tokenizer_obj.batch_encode_plus(\n",
    "        texts_list, max_length=max_len, padding='max_length', truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    print(\"Text encoding completed.\")\n",
    "    return encoded_output['input_ids'], encoded_output['attention_mask']\n",
    "\n",
    "# --- 3. Prepare Data Loaders (Dataloaders) ---\n",
    "if len(labeled_data_for_training) > 0:\n",
    "    final_train_df, final_val_df = train_test_split(\n",
    "        labeled_data_for_training, test_size=VALIDATION_SET_SIZE, random_state=RANDOM_STATE, stratify=labeled_data_for_training['class']\n",
    "    )\n",
    "    print(f\"Labeled data split into: {len(final_train_df)} training data, {len(final_val_df)} validation data.\")\n",
    "    train_inputs, train_masks, train_labels = encode_data_with_labels(tokenizer, final_train_df)\n",
    "    val_inputs, val_masks, val_labels = encode_data_with_labels(tokenizer, final_val_df)\n",
    "    final_train_dataset = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "    final_train_dataloader = DataLoader(final_train_dataset, batch_size=BATCH_SIZE_TRAIN_VAL, shuffle=True)\n",
    "    final_val_dataset = TensorDataset(val_inputs, val_masks, val_labels)\n",
    "    final_val_dataloader = DataLoader(final_val_dataset, batch_size=BATCH_SIZE_TRAIN_VAL)\n",
    "else:\n",
    "    print(\"Error: No valid labeled data available for training.\")\n",
    "    raise ValueError(\"No labeled data available.\")\n",
    "\n",
    "# --- 4. Train Model ---\n",
    "print(f\"\\n--- Training final model ({MODEL_DISPLAY_NAME}) using labeled data ---\")\n",
    "try:\n",
    "    final_model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(label_mapping)).to(device)\n",
    "    print(f\"Model ({MODEL_NAME}) initialized successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: Failed to initialize model: {e}\")\n",
    "    raise\n",
    "final_optimizer = AdamW(final_model.parameters(), lr=LEARNING_RATE)\n",
    "final_total_steps = len(final_train_dataloader) * MAX_EPOCHS\n",
    "final_scheduler = get_linear_schedule_with_warmup(\n",
    "    final_optimizer, num_warmup_steps=int(0.1 * final_total_steps), num_training_steps=final_total_steps\n",
    ")\n",
    "best_val_loss_final = float('inf')\n",
    "wait_final = 0\n",
    "best_model_weights_final = None\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "    final_model.train()\n",
    "    total_train_loss_final = 0\n",
    "    for batch in final_train_dataloader:\n",
    "        batch_to_device = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch_to_device\n",
    "        final_model.zero_grad()\n",
    "        outputs = final_model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = outputs.loss\n",
    "        total_train_loss_final += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(final_model.parameters(), 1.0)\n",
    "        final_optimizer.step()\n",
    "        final_scheduler.step()\n",
    "    avg_train_loss_final = total_train_loss_final / len(final_train_dataloader)\n",
    "    final_model.eval()\n",
    "    total_val_loss_final = 0.0\n",
    "    val_preds_final = []\n",
    "    val_labels_final_list = []\n",
    "    with torch.no_grad():\n",
    "        for batch in final_val_dataloader:\n",
    "            batch_to_device = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_mask, b_labels = batch_to_device\n",
    "            outputs = final_model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n",
    "            total_val_loss_final += outputs.loss.item()\n",
    "            logits = outputs.logits\n",
    "            val_preds_final.extend(torch.argmax(logits, dim=-1).cpu().numpy())\n",
    "            val_labels_final_list.extend(b_labels.cpu().numpy())\n",
    "    avg_val_loss_final = total_val_loss_final / len(final_val_dataloader)\n",
    "    val_f1_final = f1_score(val_labels_final_list, val_preds_final, average='weighted', zero_division=0)\n",
    "    val_accuracy_final = accuracy_score(val_labels_final_list, val_preds_final)\n",
    "    print(f\"Epoch {epoch+1}/{MAX_EPOCHS} | Training Loss: {avg_train_loss_final:.4f} | Validation Loss: {avg_val_loss_final:.4f} | Validation F1: {val_f1_final:.4f} | Accuracy: {val_accuracy_final:.4f}\")\n",
    "    if avg_val_loss_final < best_val_loss_final:\n",
    "        best_val_loss_final = avg_val_loss_final\n",
    "        wait_final = 0\n",
    "        best_model_weights_final = copy.deepcopy(final_model.state_dict())\n",
    "        print(f\"Validation loss improved. Saving model for epoch {epoch+1}\")\n",
    "    else:\n",
    "        wait_final += 1\n",
    "        print(f\"Validation loss did not improve for {wait_final} epochs.\")\n",
    "    if wait_final >= PATIENCE:\n",
    "        print(f\"Early stopping triggered for final model training after {epoch+1} epochs.\")\n",
    "        break\n",
    "if best_model_weights_final:\n",
    "    final_model.load_state_dict(best_model_weights_final)\n",
    "    print(\"Loaded best weights for final model based on validation performance.\")\n",
    "else:\n",
    "    print(\"Warning: Validation loss did not improve, using model state from last epoch (or initial).\")\n",
    "\n",
    "# --- 5. Predict for Unlabeled Data ---\n",
    "predicted_unlabeled_df = None\n",
    "if original_unlabeled_count > 0:\n",
    "    unlabeled_texts_for_prediction = unlabeled_df_original_cols['text'].tolist()\n",
    "    pred_inputs, pred_masks = encode_data_for_prediction(tokenizer, unlabeled_texts_for_prediction)\n",
    "    pred_dataset = TensorDataset(pred_inputs, pred_masks)\n",
    "    pred_dataloader = DataLoader(pred_dataset, batch_size=BATCH_SIZE_PREDICT)\n",
    "    print(f\"\\n--- Predicting classes for {original_unlabeled_count} unlabeled data using {MODEL_DISPLAY_NAME} model ---\")\n",
    "    final_model.eval()\n",
    "    all_unlabeled_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(pred_dataloader):\n",
    "            b_input_ids, b_input_mask = tuple(t.to(device) for t in batch)\n",
    "            outputs = final_model(b_input_ids, attention_mask=b_input_mask)\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            all_unlabeled_predictions.extend(predictions.cpu().numpy())\n",
    "            if (batch_idx + 1) % 100 == 0:\n",
    "                 processed_count = (batch_idx + 1) * pred_dataloader.batch_size\n",
    "                 print(f\"Predicted {min(processed_count, original_unlabeled_count)} / {original_unlabeled_count} unlabeled data...\")\n",
    "    print(f\"Prediction completed for all {original_unlabeled_count} unlabeled data.\")\n",
    "    \n",
    "    predicted_unlabeled_df = unlabeled_df_original_cols.copy()\n",
    "    predicted_unlabeled_df['class'] = all_unlabeled_predictions \n",
    "\n",
    "    print(\"\\nDistribution of predicted classes in unlabeled data (0, 1, 2) stored in 'class' column:\")\n",
    "    print(\"Percentage distribution:\")\n",
    "    print(predicted_unlabeled_df['class'].value_counts(normalize=True).sort_index())\n",
    "    print(\"Count distribution:\")\n",
    "    print(predicted_unlabeled_df['class'].value_counts().sort_index())\n",
    "else:\n",
    "    print(\"No valid unlabeled data available for prediction.\")\n",
    "\n",
    "# --- 6. Prepare and Merge Data ---\n",
    "labeled_data_final_merge = labeled_data_full_original_cols.copy()\n",
    "if 'class' in labeled_data_final_merge.columns:\n",
    "    # IMPORTANT: We use the original 'class' column from the CSV for labeled data before mapping for training\n",
    "    # This 'class' column (e.g. -1, 1, 9) from the original file will be used directly later for reverse mapping.\n",
    "    # For the purpose of merging, we need to ensure it's consistent if we were to do any operations on it *before* reverse mapping.\n",
    "    # However, since the request is to map *back* at the end, we can keep its original form (or a copy of it)\n",
    "    # and apply the label_mapping ({-1:0, 1:1, 9:2}) only for training, and then the reverse_label_mapping before saving.\n",
    "\n",
    "    # For merging, we will first get the 0,1,2 mapping for labeled data to be consistent during concat if needed.\n",
    "    # But the *final* output will use reverse mapping.\n",
    "    \n",
    "    # Let's create a temporary 'class_mapped_for_merge' for labeled data\n",
    "    # The original 'class' column from labeled_data_full_original_cols is kept for reverse mapping later.\n",
    "    temp_labeled_for_merge = labeled_data_full_original_cols.copy()\n",
    "    temp_labeled_for_merge.dropna(subset=['class'], inplace=True) # Original class values\n",
    "    temp_labeled_for_merge['class_numeric'] = temp_labeled_for_merge['class'].map(label_mapping) # Mapped to 0,1,2\n",
    "    temp_labeled_for_merge.dropna(subset=['class_numeric'], inplace=True)\n",
    "    if not temp_labeled_for_merge.empty:\n",
    "         temp_labeled_for_merge['class_numeric'] = temp_labeled_for_merge['class_numeric'].astype(int)\n",
    "    temp_labeled_for_merge['data_source'] = 'labeled_data'\n",
    "    # `temp_labeled_for_merge` now has 'class' (original e.g. -1,1,9) and 'class_numeric' (0,1,2)\n",
    "\n",
    "else:\n",
    "    print(\"Warning: 'class' column missing in original labeled data. It cannot be processed for merging or reverse mapping.\")\n",
    "    temp_labeled_for_merge = pd.DataFrame({'data_source': ['labeled_data'] * len(labeled_data_full_original_cols)}) # create a placeholder\n",
    "    # Add other necessary columns if they exist in labeled_data_full_original_cols and are expected in merge\n",
    "    for col in labeled_data_full_original_cols.columns:\n",
    "        if col not in temp_labeled_for_merge.columns:\n",
    "            temp_labeled_for_merge[col] = labeled_data_full_original_cols[col]\n",
    "    temp_labeled_for_merge['class'] = np.nan # Ensure 'class' column exists for merging, will be NaN\n",
    "    temp_labeled_for_merge['class_numeric'] = np.nan\n",
    "\n",
    "\n",
    "merged_df = pd.DataFrame() \n",
    "\n",
    "if predicted_unlabeled_df is not None:\n",
    "    unlabeled_data_final_merge = predicted_unlabeled_df.copy() \n",
    "    # 'class' column in unlabeled_data_final_merge contains predictions (0,1,2)\n",
    "    unlabeled_data_final_merge.rename(columns={'class': 'class_numeric'}, inplace=True) # rename to 'class_numeric' for consistency\n",
    "    unlabeled_data_final_merge['data_source'] = 'unlabeled_predicted'\n",
    "    # We will add an empty 'class' column here that will be filled by reverse mapping later\n",
    "    unlabeled_data_final_merge['class'] = np.nan \n",
    "\n",
    "    print(f\"\\nPreparing to merge labeled data and predicted unlabeled data.\")\n",
    "    # Select relevant columns for labeled data to merge: original columns + 'class_numeric' + 'data_source'\n",
    "    # Keep original 'class' from temp_labeled_for_merge as it is (e.g. -1, 1, 9)\n",
    "    cols_to_merge_labeled = list(labeled_data_full_original_cols.columns) + ['class_numeric', 'data_source']\n",
    "    # Ensure 'class_numeric' is in temp_labeled_for_merge (it should be if 'class' was present)\n",
    "    if 'class_numeric' not in temp_labeled_for_merge.columns and 'class' in temp_labeled_for_merge.columns: # handle case where 'class' existed but mapping failed\n",
    "        temp_labeled_for_merge['class_numeric'] = np.nan\n",
    "\n",
    "\n",
    "    merged_df = pd.concat([\n",
    "        temp_labeled_for_merge, # Has original 'class' and 'class_numeric'\n",
    "        unlabeled_data_final_merge # Has 'class_numeric' (predictions) and NaN 'class'\n",
    "    ], ignore_index=True, sort=False)\n",
    "    print(f\"Data merging completed. Total records after merging: {len(merged_df)}\")\n",
    "\n",
    "elif not temp_labeled_for_merge.empty and 'class_numeric' in temp_labeled_for_merge.columns and not temp_labeled_for_merge['class_numeric'].isna().all():\n",
    "    print(\"Only labeled data available (with valid classes), no unlabeled data predicted. Merged file will contain only processed labeled data.\")\n",
    "    merged_df = temp_labeled_for_merge.copy() \n",
    "else:\n",
    "    print(\"Warning: No labeled data with valid classes and no predicted unlabeled data to merge and save. Output file will not be generated.\")\n",
    "\n",
    "# --- 7. Reverse Map and Save Merged Data ---\n",
    "if not merged_df.empty:\n",
    "    # Define the reverse mapping\n",
    "    reverse_label_mapping = {v: k for k, v in label_mapping.items()} # {0: -1, 1: 1, 2: 9}\n",
    "    \n",
    "    print(\"\\nReverse mapping classes back to original labels (-1, 1, 9)...\")\n",
    "\n",
    "    # For labeled data, the 'class' column already holds original values (-1, 1, 9) or NaN.\n",
    "    # No reverse mapping needed for the 'class' column of labeled data.\n",
    "    # Ensure it's integer if not NaN.\n",
    "    labeled_mask = merged_df['data_source'] == 'labeled_data'\n",
    "    if 'class' in merged_df.columns:\n",
    "        # Convert to float first to handle potential NaNs, then to Int64 (pandas nullable integer)\n",
    "        merged_df.loc[labeled_mask, 'class'] = pd.to_numeric(merged_df.loc[labeled_mask, 'class'], errors='coerce').astype('Int64')\n",
    "\n",
    "\n",
    "    # For unlabeled_predicted data, 'class_numeric' holds 0,1,2. Map these to -1,1,9 in the 'class' column.\n",
    "    unlabeled_mask = merged_df['data_source'] == 'unlabeled_predicted'\n",
    "    if 'class_numeric' in merged_df.columns:\n",
    "        merged_df.loc[unlabeled_mask, 'class'] = merged_df.loc[unlabeled_mask, 'class_numeric'].map(reverse_label_mapping)\n",
    "        # Convert to float first to handle potential NaNs from mapping, then to Int64\n",
    "        merged_df.loc[unlabeled_mask, 'class'] = pd.to_numeric(merged_df.loc[unlabeled_mask, 'class'], errors='coerce').astype('Int64')\n",
    "    \n",
    "    # Drop the temporary 'class_numeric' column as it's no longer needed\n",
    "    if 'class_numeric' in merged_df.columns:\n",
    "        merged_df.drop(columns=['class_numeric'], inplace=True)\n",
    "\n",
    "    print(\"Reverse mapping completed. The 'class' column now contains original labels or NaNs.\")\n",
    "\n",
    "    try:\n",
    "        merged_df.to_csv(MERGED_OUTPUT_FILENAME, index=False)\n",
    "        print(f\"\\nMerged data with original class labels saved to {MERGED_OUTPUT_FILENAME}\")\n",
    "        print(\"\\nDistribution of 'data_source' in merged data:\")\n",
    "        print(merged_df['data_source'].value_counts())\n",
    "\n",
    "        if 'class' in merged_df.columns:\n",
    "            print(\"\\nDistribution of 'class' (original labels) in labeled_data portion of merged data:\")\n",
    "            if not merged_df[merged_df['data_source'] == 'labeled_data'].empty:\n",
    "                print(merged_df[merged_df['data_source'] == 'labeled_data']['class'].value_counts(dropna=False).sort_index())\n",
    "            else:\n",
    "                print(\"No labeled data in merged set.\")\n",
    "\n",
    "            print(\"\\nDistribution of 'class' (original labels) in unlabeled_predicted portion of merged data:\")\n",
    "            if not merged_df[merged_df['data_source'] == 'unlabeled_predicted'].empty:\n",
    "                print(merged_df[merged_df['data_source'] == 'unlabeled_predicted']['class'].value_counts(dropna=False).sort_index())\n",
    "            else:\n",
    "                print(\"No unlabeled_predicted data in merged set.\")\n",
    "                \n",
    "            print(\"\\nTotal non-null values in 'class' column of merged data (original labels):\")\n",
    "            print(merged_df['class'].notna().sum())\n",
    "            print(\"\\nOverall distribution of 'class' (original labels) in merged data:\")\n",
    "            print(merged_df['class'].value_counts(dropna=False).sort_index())\n",
    "        else:\n",
    "            print(\"\\n'class' column not found in the final merged_df.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error: Failed to save merged CSV file: {e}\")\n",
    "else:\n",
    "    print(f\"File {MERGED_OUTPUT_FILENAME} not generated due to no data to process.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdcdcc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
